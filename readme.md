## Экстрактивная суммаризация на основе вхождения общих слов

Данный алгоритм очень прост. 
Реализация не имеет никаких зависимостей, кроме стандартной библиотеки python3. 
Должно работать на python3.8+, но может и ниже. 
Здесь мы работаем только с исходным текстом.  
Извлекаемые информационные блоки будут представлять собой определенные предложения текста.

- На первом шаге разбиваем входной текст на предложения простой эвристикой на основе регулярного выражения.
- Каждое предложение разбиваем на токены (отдельные слова), проводим для них грубый стемминг, оставляя первые 4 символа, короткие слова (1-2 буквы )и стоп слова выбрасываем.
- Затем задаем функцию схожести для каждой пары предложений. 
- Она будет рассчитываться как отношение числа общих слов, встречающихся в обоих предложениях, к общему количеству уникальных слов в них длине. 
- В результате мы получим коэффициенты схожести для каждой пары предложений и составим матрицу схожести.
- Коэффициент схожести предложения с самим собой считаем 0.
- Затем для каждого предложения переходим к относительным коэффициентам схожести (считаем сумму коэффициентов для каждого предложения за 1 и вычисляем доли схожести. - Нормирование по строкам.)
- Для каждого предложения вычисляем скор - суммарную относительную похожесть на другие предложения - суммирование по столбцам.
- Оставляем предложения с наибольшим скором - условно, те которые относительно похожи на наибольшее число предложений, те имеют больше пересечений с ними по ключевым словам.
- Выводим отобранные предложения в том порядке, в котором они встречаются в тексте.

Основная функция модуля summarize

```python3
from extractsum_simple.summarize_simple import summarize
text_sum: str = summarize(text)
```

Если не указать никаких дополнительных параметров, то извлечется всего корень из количества предложений в тексте + 9. Например, если текст из 100 предложений, то извлечется 19.

С параметрами по умолчанию короткие тексты (12 предложений и менее) отдаются полностью с учетом изменения форматирования - каждое предложение станет с новой строки.

Можно задать сколько предложений нужно извлечь явно

```python3
from extractsum_simple.summarize_simple import summarize

# извлечь 16 предложений, 3 первых, 10 изнутри документы, 3 последних, использовать стоп слова по умолчанию
text_sum1 = summarize(text, n=10, first_n=3, last_n=3)  

# извлечь корень из колиечества предложений текста + 3 (например 7 из 16, 13 из 100), не использовать стоп слова, не обязательно брать первые и последние предложения
text_sum1 = summarize(text, first_n=0, last_n=0, stop_words=[])  

```

- ```text``` - строка
- ```n``` - сколько предложений извлечь из середины текста, если не задано, то вычисляется как ```n = math.ceil(math.sqrt(len(sents) - first_n - last_n)) + 3```
- ```first_n``` - сколько предложений взять из начала текста
- ```last_n``` - сколько предложений взять из конца текста
- будет извлечен текст состоящий их ```first_n + n + last_n``` предложений
- по умолчанию заданы параметры ```n = None, first_n = 4, last_n = 2```
- возвращается текст в котором предложения объединены через перенос строки ```'\n'```
- также может принимать аргумент ```stop_words: List[str]``` - список стоп слов, которые можно удалить перед токенизацией
- задавайте все стоп слова только в нижнем регистре
- передайте пустой список стоп слов ```stop_words=[]```, чтобы не использовать стоп слова по умолчанию

Также можно использовать другие функции модуля
- ```extract_sentences(text: str)-> List[str]``` - разбить текст на предложения
- ```tokenize(text: str, stop_words: List[str]=stop_words)-> List[str]``` - разбить предложения на токены, условно на корни слов, выбросив стоп-слова
- ```extract_sum(text: str, n: int, stop_words:List[str]=stop_words)``` - извлечь n самых информативных предложений в виде списка
